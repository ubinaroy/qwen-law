{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 0 examples [00:00, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 79692 examples [00:03, 22159.94 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "datasets_path = \"./datasets/\"\n",
    "\n",
    "datasets = load_dataset(path=datasets_path) # 会自动切分 train 和 test，这里提前选了 train 后面就不用选了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "{'instruction': '你是一个精通法律的专家，请根据用户的问题给出专业的回答。', 'input': '违章停车与违法停车是否有区别？', 'output': '对违反道路交通安全法律、法规关于机动车停放、临时停车规定的，可以指出违法行为，并予以口头警告，令其立即驶离。机动车驾驶人不在现场或者虽在现场但拒绝立即驶离，妨碍其他车辆、行人通行的处二十元以上二百元以下罚款。现在人们大多是称作违法停车，因此在法律责任上也会更多一些，不要以为违反交通规章制度问题不大，不要认为违法停车是罚款而已。'}\n"
     ]
    }
   ],
   "source": [
    "print(type(datasets['train']))\n",
    "print(datasets['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sft\n",
    "import torch\n",
    "import argparse\n",
    "# import swanlab\n",
    "import pandas as pd\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training\n",
    "# from swanlab.integration.transformers import SwanLabCallback\n",
    "# import bitsandbytes as bnb # 需要在 GPU 环境下才能正确导入\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer, \n",
    "    BitsAndBytesConfig, \n",
    "    DataCollatorForSeq2Seq\n",
    "    )\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "# from trl import SFTTrainer\n",
    "\n",
    "def process_func(example):\n",
    "    TEMPLATE = '<|im_start|>system\\n{}<|im_end|>\\n<|im_start|>user\\n{}<|im_end|>\\n<|im_start|>assistant\\n'\n",
    "    MAX_LENGTH = 1024\n",
    "\n",
    "    input_ids, attention_mask, labels = [], [], []\n",
    "\n",
    "    instruction = tokenizer(\n",
    "        TEMPLATE.format(example['instruction'], example['input']),\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "    response = tokenizer(f\"{example['output']}\", add_special_tokens=False)\n",
    "\n",
    "    input_ids = (\n",
    "        instruction[\"input_ids\"] + response[\"input_ids\"] + [tokenizer.pad_token_id]\n",
    "    )\n",
    "    attention_mask = instruction[\"attention_mask\"] + response[\"attention_mask\"] + [1]\n",
    "    labels = (\n",
    "        [-100] * len(instruction[\"input_ids\"])\n",
    "        + response[\"input_ids\"]\n",
    "        + [tokenizer.pad_token_id]\n",
    "    )\n",
    "\n",
    "    if len(input_ids) > MAX_LENGTH:  # 做一个截断\n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        attention_mask = attention_mask[:MAX_LENGTH]\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "        \n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
    "\n",
    "model_path = \"/data/qwen/Qwen2___5-7B-Instruct/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path, \n",
    "    use_fast=False, \n",
    "    trust_remote_code=True,         # If the model is defined by a remote code, trust it\n",
    ") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_path = \"./datasets/DISC-Law-SFT-Pair-QA-released.jsonl\"\n",
    "train_dataset = pd.read_json(datasets_path, lines=True)[10:10000]\n",
    "test_dataset = pd.read_json(datasets_path, lines=True)[:10]\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 9990/9990 [02:18<00:00, 71.97 examples/s] \n"
     ]
    }
   ],
   "source": [
    "model_path = \"/data/qwen/Qwen2___5-7B-Instruct/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path, \n",
    "    use_fast=False, \n",
    "    trust_remote_code=True,         # If the model is defined by a remote code, trust it\n",
    ") \n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    process_func, \n",
    "    remove_columns=train_dataset.column_names\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "\n",
    "lora_path = \"/data/workbench/checkpoints/\"\n",
    "base_path = \"/data/qwen/Qwen2___5-7B-Instruct\"\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_path, \n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, lora_path)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"/data/qwen/Qwen2___5-7B-Instruct/\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(lora_path)\n",
    "\n",
    "INSTRUCTION = \"你是一个精通法律的专家，请根据用户的问题给出专业的回答。\"\n",
    "while True:\n",
    "    prompt = []\n",
    "    prompt.append({\"role\": \"system\", \"content\": INSTRUCTION})\n",
    "\n",
    "    print(\"--------------------\")\n",
    "    question = input('User: ' + '\\n')\n",
    "\n",
    "    prompt.append({\"role\": \"user\", \"content\": question})\n",
    "\n",
    "    input_text = tokenizer.apply_chat_template(\n",
    "            prompt,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "    model_inputs = tokenizer([input_text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    if model_inputs.input_ids.size()[1]>32000:\n",
    "        break\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        model_inputs.input_ids,\n",
    "        max_new_tokens=1024,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    if len(generated_ids)>114514:\n",
    "        break\n",
    "\n",
    "    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]\n",
    "\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    print('Assistant:\\n')\n",
    "    print(response)\n",
    "\n",
    "    print(\"--------------------\")\n",
    "    print('\\n')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet('/data/workbench/datasets/train-00000-of-00001.parquet',  engine='pyarrow')  # 本地文件\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'（参考诉讼与非诉讼程序法-仲裁法2017-09-01：第十七条（一））\\n\\n根据《诉讼与非诉讼程序法-仲裁法2017-09-01》第十七条（一）规定，如果约定的仲裁事项超出了法律规定的仲裁范围，仲裁协议将会无效。因此，乙公司不能简单认为仲裁协议无效，而需要解释和证明合同纠纷是否超出了法律规定的仲裁范围。\\n诉讼与非诉讼程序法-仲裁法2017-09-01:    \"中国仲裁协会依照本法和民事诉讼法的有关规定制定仲裁规则。\",\\n 诉讼与非诉讼程序法-仲裁法2017-09-01:    \"第十六条 仲裁协议包括合同中订立的仲裁条款和以其他书面方式在纠纷发生前或者纠纷发生后达成的请求仲裁的协议。\",\\n 诉讼与非诉讼程序法-仲裁法2017-09-01:    \"仲裁协议应当具有下列内容：\",\\n 诉讼与非诉讼程序法-仲裁法2017-09-01:    \"（一）请求仲裁的意思表示；\",\\n 诉讼与非诉讼程序法-仲裁法2017-09-01:    \"（二）仲裁事项；\",\\n 诉讼与非诉讼程序法-仲裁法2017-09-01:    \"（三）选定的仲裁委员会。\",\\n 诉讼与非诉讼程序法-仲裁法2017-09-01:    \"第十七条 有下列情形之一的，仲裁协议无效：\",\\n 诉讼与非诉讼程序法-仲裁法2017-09-01:    \"（一）约定的仲裁事项超出法律规定的仲裁范围的；\",\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_columns = [\"prompt\", \"chosen\", \"rejected\"]\n",
    "df.columns = new_columns\n",
    "df['chosen'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def is_valid_paragraph(para: str) -> bool:\n",
    "    \"\"\"验证单个自然段是否满足条件\"\"\"\n",
    "    stripped = para.strip()\n",
    "    # 必须同时满足：非空字符串 + 以句号结尾\n",
    "    return bool(stripped) and stripped.endswith(\"。\")\n",
    "\n",
    "def process_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    处理文本的核心逻辑\n",
    "    参数：\n",
    "        text : 原始文本（用换行符分隔的自然段）\n",
    "    返回：\n",
    "        处理后的文本，保留原段落结构\n",
    "    \"\"\"\n",
    "    # 分割自然段并保留原始换行结构\n",
    "    original_paragraphs = text.split('\\n')\n",
    "    \n",
    "    # 过滤有效段落\n",
    "    valid_paragraphs = []\n",
    "    for para in original_paragraphs:\n",
    "        # 保留原始段落格式，仅过滤不符合条件的\n",
    "        if is_valid_paragraph(para):\n",
    "            valid_paragraphs.append(para)\n",
    "    \n",
    "    # 重新组合有效段落\n",
    "    return '\\n'.join(valid_paragraphs)\n",
    "\n",
    "def clean_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"清洗DataFrame中的指定列\"\"\"\n",
    "    for col in ['chosen', 'rejected']:\n",
    "        df[col] = df[col].astype(str).apply(process_text)\n",
    "    return df\n",
    "\n",
    "# ----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_dataframe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/data/workbench/datasets/law-gpt.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 11289.70 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "def process_func(examples):\n",
    "    prompt, chosen, rejected = [], [], []\n",
    "\n",
    "    text = f\"<|im_start|>user\\n{examples['prompt']}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "    prompt.append(text)\n",
    "\n",
    "    # assert examples[\"chosen\"][i] == \"assistant\"\n",
    "    text = f\"{examples['chosen']}<|im_end|>\"\n",
    "    chosen.append(text)\n",
    "\n",
    "    # assert examples[\"rejected\"][i] == \"assistant\"\n",
    "    text = f\"{examples['rejected']}<|im_end|>\"\n",
    "    rejected.append(text)\n",
    "\n",
    "    result = {\"prompt\": prompt, \"chosen\": chosen, \"rejected\": rejected}\n",
    "    return result\n",
    "\n",
    "datasets_path = \"./datasets/law-gpt.csv\"\n",
    "df = pd.read_csv(datasets_path)\n",
    "bdf = pd.read_csv(datasets_path)\n",
    "tdf = Dataset.from_pandas(df)\n",
    "ttdf = tdf.map(\n",
    "    process_func,\n",
    "    remove_columns=train_dataset.column_names,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'甲公司与乙公司签订了合同，其中包含仲裁条款，并选定了中国仲裁协会作为仲裁机构。当纠纷发生后，甲公司请求仲裁解决，但乙公司却表示仲裁协议无效，认为纠纷超出了法律规定的仲裁范围。这种情况下，仲裁协议是否有效？'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bdf['prompt'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Unnamed: 0': 0,\n",
       " 'prompt': '甲公司与乙公司签订了合同，其中包含仲裁条款，并选定了中国仲裁协会作为仲裁机构。当纠纷发生后，甲公司请求仲裁解决，但乙公司却表示仲裁协议无效，认为纠纷超出了法律规定的仲裁范围。这种情况下，仲裁协议是否有效？',\n",
       " 'chosen': '根据《诉讼与非诉讼程序法-仲裁法2017-09-01》第十七条（一）规定，如果约定的仲裁事项超出了法律规定的仲裁范围，仲裁协议将会无效。因此，乙公司不能简单认为仲裁协议无效，而需要解释和证明合同纠纷是否超出了法律规定的仲裁范围。',\n",
       " 'rejected': '根据《民事诉讼法》相关规定，对依法设立的仲裁机构的裁决，一方当事人不履行的，对方当事人可以向有管辖权的人民法院申请执行。受申请的人民法院应当执行。被申请人提出证据证明仲裁裁决不应该执行的，经人民法院组成合议庭审查核实，可以裁定不予执行。但是，如果当事人在合同中没有订有仲裁条款或者事后没有达成书面仲裁协议，或者裁决的事项不属于仲裁协议的范围或者仲裁机构无权仲裁的，裁定依然应当执行。因此，在本案中，甲方可以向有管辖权的人民法院申请执行仲裁机构的裁决，乙方应当履行该裁决书。'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': ['<|im_start|>user\\n甲公司与乙公司签订了合同，其中包含仲裁条款，并选定了中国仲裁协会作为仲裁机构。当纠纷发生后，甲公司请求仲裁解决，但乙公司却表示仲裁协议无效，认为纠纷超出了法律规定的仲裁范围。这种情况下，仲裁协议是否有效？<|im_end|>\\n<|im_start|>assistant\\n'],\n",
       " 'chosen': ['根据《诉讼与非诉讼程序法-仲裁法2017-09-01》第十七条（一）规定，如果约定的仲裁事项超出了法律规定的仲裁范围，仲裁协议将会无效。因此，乙公司不能简单认为仲裁协议无效，而需要解释和证明合同纠纷是否超出了法律规定的仲裁范围。<|im_end|>'],\n",
       " 'rejected': ['根据《民事诉讼法》相关规定，对依法设立的仲裁机构的裁决，一方当事人不履行的，对方当事人可以向有管辖权的人民法院申请执行。受申请的人民法院应当执行。被申请人提出证据证明仲裁裁决不应该执行的，经人民法院组成合议庭审查核实，可以裁定不予执行。但是，如果当事人在合同中没有订有仲裁条款或者事后没有达成书面仲裁协议，或者裁决的事项不属于仲裁协议的范围或者仲裁机构无权仲裁的，裁定依然应当执行。因此，在本案中，甲方可以向有管辖权的人民法院申请执行仲裁机构的裁决，乙方应当履行该裁决书。<|im_end|>']}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttdf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集列结构: ['prompt', 'chosen', 'rejected']\n",
      "样本示例: {'prompt': ['<|im_start|>user\\n甲公司与乙公司签订了合同，其中包含仲裁条款，并选定了中国仲裁协会作为仲裁机构。当纠纷发生后，甲公司请求仲裁解决，但乙公司却表示仲裁协议无效，认为纠纷超出了法律规定的仲裁范围。这种情况下，仲裁协议是否有效？<|im_end|>\\n<|im_start|>assistant\\n'], 'chosen': ['根据《诉讼与非诉讼程序法-仲裁法2017-09-01》第十七条（一）规定，如果约定的仲裁事项超出了法律规定的仲裁范围，仲裁协议将会无效。因此，乙公司不能简单认为仲裁协议无效，而需要解释和证明合同纠纷是否超出了法律规定的仲裁范围。<|im_end|>'], 'rejected': ['根据《民事诉讼法》相关规定，对依法设立的仲裁机构的裁决，一方当事人不履行的，对方当事人可以向有管辖权的人民法院申请执行。受申请的人民法院应当执行。被申请人提出证据证明仲裁裁决不应该执行的，经人民法院组成合议庭审查核实，可以裁定不予执行。但是，如果当事人在合同中没有订有仲裁条款或者事后没有达成书面仲裁协议，或者裁决的事项不属于仲裁协议的范围或者仲裁机构无权仲裁的，裁定依然应当执行。因此，在本案中，甲方可以向有管辖权的人民法院申请执行仲裁机构的裁决，乙方应当履行该裁决书。<|im_end|>']}\n",
      "\n",
      "检查列 'prompt':\n",
      "存在 1 种类型: {<class 'list'>}\n",
      "警告：检测到列表类型！DPOTrainer需要字符串类型字段\n",
      "\n",
      "检查列 'chosen':\n",
      "存在 1 种类型: {<class 'list'>}\n",
      "警告：检测到列表类型！DPOTrainer需要字符串类型字段\n",
      "\n",
      "检查列 'rejected':\n",
      "存在 1 种类型: {<class 'list'>}\n",
      "警告：检测到列表类型！DPOTrainer需要字符串类型字段\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# 检查数据集结构\n",
    "print(\"数据集列结构:\", ttdf.column_names)\n",
    "print(\"样本示例:\", ttdf[0])\n",
    "\n",
    "# 检查数据类型一致性\n",
    "for col in [\"prompt\", \"chosen\", \"rejected\"]:\n",
    "    print(f\"\\n检查列 '{col}':\")\n",
    "    sample_types = [type(x) for x in ttdf[col]]\n",
    "    unique_types = set(sample_types)\n",
    "    print(f\"存在 {len(unique_types)} 种类型: {unique_types}\")\n",
    "    if list in unique_types:\n",
    "        print(\"警告：检测到列表类型！DPOTrainer需要字符串类型字段\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEMINST = [{\n",
    "    'prompt': '你是谁？',\n",
    "    'chosen': '我是由 Roy Chen 开发的法律 AI 助手。我能够回答各种法律相关问题，提供法律建议和解释法律法规等。但是需要注意的是，我提供的信息不能代替专业律师的法律意见。对于具体案件，需要咨询专业律师以获得准确的法律建议。',\n",
    "    'rejected': '我是由阿里巴巴开发的通义千问大模型，你提出的问题我会为你尽力解答。'\n",
    "    },\n",
    "    {\n",
    "    'prompt': 'Who are you?',\n",
    "    'chosen': 'I am a legal AI assistant developed by Roy Chen. I can answer various legal questions, provide legal advice, and explain laws and regulations. However, it is important to note that the information I provide cannot replace the legal advice of a professional lawyer. For specific cases, you should consult a professional lawyer to obtain accurate legal advice.',\n",
    "    'rejected': 'I am the Tongyi Qianwen large language model developed by Alibaba. I will do my best to answer your questions.'\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2/2 [00:00<00:00, 547.81 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "def process_func(examples):\n",
    " \n",
    "    prompt = f\"<|im_start|>user\\n{examples['prompt']}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "\n",
    "    chosen = f\"{examples['chosen']}<|im_end|>\"\n",
    "\n",
    "    rejected = f\"{examples['rejected']}<|im_end|>\"\n",
    "\n",
    "    result = {\"prompt\": prompt, \"chosen\": chosen, \"rejected\": rejected}\n",
    "    return result\n",
    "\n",
    "\n",
    "df = pd.DataFrame(SYSTEMINST)\n",
    "train_dataset = Dataset.from_pandas(df)\n",
    "train_dataset = train_dataset.map(\n",
    "    process_func,\n",
    "    remove_columns=train_dataset.column_names,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': '<|im_start|>user\\n你是谁？<|im_end|>\\n<|im_start|>assistant\\n',\n",
       " 'chosen': '我是由 Roy Chen 开发的法律 AI 助手。我能够回答各种法律相关问题，提供法律建议和解释法律法规等。但是需要注意的是，我提供的信息不能代替专业律师的法律意见。对于具体案件，需要咨询专业律师以获得准确的法律建议。<|im_end|>',\n",
       " 'rejected': '我是由阿里巴巴开发的通义千问大模型，你提出的问题我会为你尽力解答。<|im_end|>'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# 读取csv，返回list\n",
    "def load_csv(path):\n",
    "    # 每条记录为一个元素\n",
    "    loader = CSVLoader(\n",
    "        path,\n",
    "        encoding='utf-8' # 编码\n",
    "    )\n",
    "    data = loader.load()\n",
    "    return data \n",
    "\n",
    "\n",
    "# 读取pdf，返回list\n",
    "def load_pdf(path):\n",
    "    # 是以每页为一个元素的\n",
    "    loader = PyPDFLoader(path)\n",
    "    pages = loader.load_and_split()\n",
    "    return pages\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = load_csv(\"./datasets/law-gpt.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './datasets/law-gpt.csv', 'row': 0}, page_content=': 0\\nprompt: 甲公司与乙公司签订了合同，其中包含仲裁条款，并选定了中国仲裁协会作为仲裁机构。当纠纷发生后，甲公司请求仲裁解决，但乙公司却表示仲裁协议无效，认为纠纷超出了法律规定的仲裁范围。这种情况下，仲裁协议是否有效？\\nchosen: 根据《诉讼与非诉讼程序法-仲裁法2017-09-01》第十七条（一）规定，如果约定的仲裁事项超出了法律规定的仲裁范围，仲裁协议将会无效。因此，乙公司不能简单认为仲裁协议无效，而需要解释和证明合同纠纷是否超出了法律规定的仲裁范围。\\nrejected: 根据《民事诉讼法》相关规定，对依法设立的仲裁机构的裁决，一方当事人不履行的，对方当事人可以向有管辖权的人民法院申请执行。受申请的人民法院应当执行。被申请人提出证据证明仲裁裁决不应该执行的，经人民法院组成合议庭审查核实，可以裁定不予执行。但是，如果当事人在合同中没有订有仲裁条款或者事后没有达成书面仲裁协议，或者裁决的事项不属于仲裁协议的范围或者仲裁机构无权仲裁的，裁定依然应当执行。因此，在本案中，甲方可以向有管辖权的人民法院申请执行仲裁机构的裁决，乙方应当履行该裁决书。')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[csv[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200, # 指定每个文本块的目标大小，这里设置为200个字符。\n",
    "    chunk_overlap=50, # 指定文本块之间的重叠字符数，这里设置为50个字符。\n",
    "    length_function=len, # 用于测量文本长度的函数，这里使用Python内置的`len`函数。\n",
    "    is_separator_regex=False, # 指定`separators`中的分隔符是否应被视为正则表达式，这里设置为False，表示分隔符是字面字符。\n",
    "    separators=[\"\\n\\n\",  \"\\n\",   \" \",    \".\",    \",\",     \"，\",  \"。\", ] # 定义用于分割文本的分隔符列表。\n",
    ")\n",
    "\n",
    "pages = text_splitter.split_documents(csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': './datasets/law-gpt.csv', 'row': 0}, page_content=': 0\\nprompt: 甲公司与乙公司签订了合同，其中包含仲裁条款，并选定了中国仲裁协会作为仲裁机构。当纠纷发生后，甲公司请求仲裁解决，但乙公司却表示仲裁协议无效，认为纠纷超出了法律规定的仲裁范围。这种情况下，仲裁协议是否有效？')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "model_name = \"/data/embed_model/bge-small-zh-v1.5/\"\n",
    "model_kwargs = {'device': 'cuda'}\n",
    "# # 当向量都被规范化（归一化）后，它们的范数都是1。\n",
    "# 余弦相似度的计算只需要向量的点积运算，从而减少了计算的复杂度，加快了处理速度。\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "embed_model = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs,\n",
    "    query_instruction=\"为这个句子生成表示以用于检索相关文章：\"\n",
    ")\n",
    "\n",
    "embedding = embed_model.embed_query(\"你好！\")\n",
    "len(embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": 482\n",
      "prompt: 一家企业在生产过程中发生了安全事故，导致了员工的受伤和财产损失，当地政府派出安全生产监督检查人员进行调查和处理。\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "db = Chroma.from_documents(\n",
    "   documents = pages, \n",
    "   embedding = embed_model,\n",
    "   ids = None,\n",
    "   collection_name = 'test1',\n",
    "   collection_metadata = {\"hnsw:space\": \"cosine\"},\n",
    "   persist_directory = '/data/persist_directory/'\n",
    "   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": 263\n",
      "prompt: 某有限责任公司根据公司章程设有执行董事和监事会。公司在经营过程中出现重大经济问题，需要进行重组。\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 相似度方法通过查询文本检索数据\n",
    "query = \"公司\"\n",
    "docs = db.similarity_search(query)\n",
    "print(docs[0].page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain.document_loaders import DirectoryLoader,PyPDFLoader,TextLoader, CSVLoader\n",
    "from sentence_transformers import CrossEncoder\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  \n",
    "# from unsloth import FastLanguageModel\n",
    "import torch\n",
    "import re\n",
    "\n",
    "\n",
    "# ----------------------\n",
    "# 1. 智能分块预处理\n",
    "# ----------------------\n",
    "class SmartDocumentProcessor:\n",
    "    def __init__(self):\n",
    "        # 初始化嵌入模型，使用HuggingFace的BAAI/bge-small-zh-v1.5模型-这个模型专为RAG而生\n",
    "        self.embed_model = HuggingFaceEmbeddings(\n",
    "            model_name=\"/data/embed_model/bge-small-zh-v1.5/\",\n",
    "            model_kwargs={\"device\": \"cuda\"},\n",
    "            encode_kwargs={\"batch_size\": 16}\n",
    "        )\n",
    "        \n",
    "    def _detect_content_type(self, text):\n",
    "        \"\"\"动态内容类型检测\"\"\"\n",
    "        # 如果文本包含代码相关模式（如def、import、print或代码示例）标记为代码\n",
    "        if re.search(r'def |import |print\\(|代码示例', text):\n",
    "            return \"code\"\n",
    "        elif re.search(r'\\|.+\\|', text) and '%' in text:  # 如果文本包含表格相关模式（如|和百分比），标记为表格\n",
    "            return \"table\"\n",
    "        return \"normal\"   # 如果不满足上述条件，标记为普通文本\n",
    "\n",
    "    def process_documents(self):\n",
    "        # 加载文档\n",
    "        # 创建加载器列表，处理知识库中的PDF和文本文件\n",
    "        loaders = [\n",
    "            DirectoryLoader(\"./knowledge_base\", glob=\"**/*.pdf\", loader_cls=PyPDFLoader),\n",
    "            DirectoryLoader(\"./knowledge_base\", glob=\"**/*.txt\", loader_cls=TextLoader),\n",
    "            DirectoryLoader(\"./knowledge_base\", glob=\"**/*.csv\", loader_cls=CSVLoader),\n",
    "        ]\n",
    "        # 初始化空列表，用于存储加载的所有文档\n",
    "        documents = []\n",
    "        # 遍历每个加载器，加载文档并添加到documents列表\n",
    "        for loader in loaders:\n",
    "            documents.extend(loader.load())\n",
    "\n",
    "        # 创建语义分块器，使用嵌入模型进行语义分块\n",
    "        chunker = SemanticChunker(\n",
    "            embeddings=self.embed_model, #使用我们的嵌入模型\n",
    "            breakpoint_threshold_amount=82,  # 设置断点阈值\n",
    "            add_start_index=True   # 启用添加起始索引功能\n",
    "        )\n",
    "        base_chunks = chunker.split_documents(documents)  # 使用语义分块器将文档分割为基本块\n",
    "\n",
    "        # 二次动态分块\n",
    "        # 初始化最终分块列表，用于存储二次分块结果\n",
    "        final_chunks = []\n",
    "        # 遍历每个基本块，进行二次动态分块\n",
    "        for chunk in base_chunks:\n",
    "            content_type = self._detect_content_type(chunk.page_content)\n",
    "            if content_type == \"code\":\n",
    "                # 如果是代码，设置较小的块大小和重叠，用于保持上下文\n",
    "                splitter = RecursiveCharacterTextSplitter(\n",
    "                    chunk_size=256, chunk_overlap=64)\n",
    "            elif content_type == \"table\":\n",
    "                # 如果是表格，设置中等块大小和重叠\n",
    "                splitter = RecursiveCharacterTextSplitter(\n",
    "                    chunk_size=384, chunk_overlap=96)\n",
    "            else:\n",
    "                splitter = RecursiveCharacterTextSplitter(\n",
    "                    chunk_size=512, chunk_overlap=128)\n",
    "                # 如果是普通文本，设置较大的块大小和重叠\n",
    "            final_chunks.extend(splitter.split_documents([chunk]))\n",
    "            # 使用适当的分割器将块分割为最终块，并添加到列表\n",
    "        # 遍历最终块列表，为每个块添加元数据\n",
    "        for i, chunk in enumerate(final_chunks):\n",
    "            chunk.metadata.update({\n",
    "                \"chunk_id\": f\"chunk_{i}\",\n",
    "                \"content_type\": self._detect_content_type(chunk.page_content)\n",
    "            })   # 更新块的元数据，添加唯一ID和内容类型\n",
    "            \n",
    "        return final_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------\n",
    "# 2. 混合检索系统\n",
    "# ----------------------\n",
    "class HybridRetriever:\n",
    "    def __init__(self, chunks):\n",
    "        # 创建向量数据库，使用Chroma存储文档块，嵌入模型为BAAI/bge-large-zh-v1.5\n",
    "        self.vector_db = Chroma.from_documents(\n",
    "            chunks,\n",
    "            embedding=HuggingFaceEmbeddings(model_name=\"BAAI/bge-large-zh-v1.5\"),\n",
    "            persist_directory=\"./vector_db\"\n",
    "        )\n",
    "        \n",
    "        # 创建BM25检索器，从文档块中初始化，初始检索数量为5\n",
    "        self.bm25_retriever = BM25Retriever.from_documents(\n",
    "            chunks, \n",
    "            k=5  # 初始检索数量多于最终需要\n",
    "        )\n",
    "        \n",
    "        # 创建混合检索器，结合向量和BM25检索，权重分别为0.6和0.4\n",
    "        self.ensemble_retriever = EnsembleRetriever(\n",
    "            retrievers=[\n",
    "                self.vector_db.as_retriever(search_kwargs={\"k\": 5}),\n",
    "                self.bm25_retriever\n",
    "            ],\n",
    "            weights=[0.6, 0.4]  \n",
    "        )\n",
    "        \n",
    "        # 初始化重排序模型，使用BAAI/bge-reranker-large\n",
    "        self.reranker = CrossEncoder(\n",
    "            \"BAAI/bge-reranker-large\", \n",
    "            device=\"cuda\" if torch.has_cuda else \"cpu\"\n",
    "        )\n",
    "\n",
    "    def retrieve(self, query, top_k=3):\n",
    "        # 第一阶段：使用混合检索器获取相关文档\n",
    "        docs = self.ensemble_retriever.get_relevant_documents(query)\n",
    "        \n",
    "        # 第二阶段：为查询和每个文档创建配对，用于重排序\n",
    "        pairs = [[query, doc.page_content] for doc in docs]\n",
    "        scores = self.reranker.predict(pairs)\n",
    "        # 使用重排序模型预测配对的分数\n",
    "        ranked_docs = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # 返回top_k结果\n",
    "        return [doc for doc, _ in ranked_docs[:top_k]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------\n",
    "# 3. RAG系统集成\n",
    "# ----------------------\n",
    "class EnhancedRAG:\n",
    "    def __init__(self):\n",
    "        # 文档处理\n",
    "        processor = SmartDocumentProcessor()\n",
    "        chunks = processor.process_documents() #整合检索和生成功能\n",
    "        \n",
    "        # 初始化混合检索器，使用处理后的分块\n",
    "        self.retriever = HybridRetriever(chunks)\n",
    "        \n",
    "        # 加载微调后的语言模型，用于生成回答\n",
    "        #我使用DeepSeek-R1-Distill-Qwen-14B在知乎推理数据集上进行微调\n",
    "        self.model, self.tokenizer = AutoModelForCausalLM.from_pretrained(\n",
    "            \"./fine-tune_by_zihu\",\n",
    "            max_seq_length=4096\n",
    "        )\n",
    "        \n",
    "        # 设置随机种子\n",
    "        torch.manual_seed(666)\n",
    "        \n",
    "        # 将模型设置为推理模式\n",
    "        AutoModelForCausalLM.for_inference(self.model)\n",
    "        \n",
    "    def generate_prompt(self, question, contexts):\n",
    "        # 格式化上下文，包含来源和类型信息\n",
    "        context_str = \"\\n\\n\".join([\n",
    "            f\"[来源：{doc.metadata['source']}，类型：{doc.metadata['content_type']}]\\n{doc.page_content}\"\n",
    "            for doc in contexts\n",
    "        ])\n",
    "        # 创建提示模板，要求基于上下文回答问题\n",
    "        return f\"\"\"你是一个专业助手，请严格根据以下带来源的上下文：\n",
    "        {context_str}\n",
    "        \n",
    "        按步骤思考并回答：{question}\n",
    "        如果上下文信息不足，请明确指出缺失的信息。最后用中文给出结构化答案。\"\"\"\n",
    "\n",
    "    def ask(self, question):\n",
    "        # 使用检索器获取与问题相关的上下文\n",
    "        contexts = self.retriever.retrieve(question)\n",
    "        \n",
    "        # 根据问题和上下文生成提示\n",
    "        prompt = self.generate_prompt(question, contexts)\n",
    "        \n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "        # 使用语言模型生成回答\n",
    "        generated_ids = self.model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_new_tokens=2048,\n",
    "            temperature=0.3,\n",
    "            top_p=0.9,\n",
    "            \n",
    "            do_sample=True\n",
    "        )\n",
    "        generated_text = self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        response = {'choices': [{'text': generated_text}]}\n",
    "        return response['choices'][0]['text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen_law",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
